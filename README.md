# Simple ETL process with KNIME

1. [Business Understanding](#Business-Understanding)
2. [Data Understanding](#Data-Understanding)
3. [Data Preparation](#Data-Preparation)
4. [Modeling](#.Modeling)
5. [Evaluation](#Evaluation)
6. [Deployment](#Deployment)
7. [Final Workflow](#Final-Workflow)

## Business Understanding

In this project, we use chess game records from Lichess.org as the dataset. Process that could be applied to this data is concatinating (append) the data because it only consist of 1 table/file.

## Data Understanding

- Number of rows: 20058
- Number of columns: 16

### Columns detail

1. **Id**: Game identifier;
2. **Rated**: Wether the game is rated or not;
3. **Created_at**: Start time;
4. **Last_move_at**: End Time;
5. **Turns**: Number of turns;
6. **Victory_status**: Game status;
7. **Winner**: wether black or white;
8. **Increment_code**: Time increment mode;
9. **White_id**: White player identifier;
10. **White_rating**: White player rating;
11. **Black_id**: Black player identifier;
12. **Black_rating**: Black player rating;
13. **Moves**: All Moves in Standard Chess Notation;
14. **Opening_eco**: Standardised Code for any given opening, [(list here)](https://www.365chess.com/eco.php);
15. **Opening_name**: Opening Name;
16. **Opening_ply**: Number of moves in the opening phase;

## Data Preparation

The dataset will be splitted into 2 part, the 19058 first rows will still be CSV file, and the rest rows will be imported to MySQL database.

1. Use text editor to move the 2nd part (100 rows) of dataset to new CSV file;
2. Save the 1st part (19058 rows) of dataset to new CSV file;
3. Read CSV in KNIME with **CSV Reader**;

    ![CSV Reader node][csvreader]
4. Add **MySQL Connector** and set username, password, hostname, and database name;

    ![MySQL Connector node][mysqlconnector]
5. Add **DB Writer** and set **MySQL Connector**'s output and **CSV Reader**'s output as input;

    ![DB Writer node][dbwriter]
6. Execute.

    - MySQL Connector configuration:

    ![MySQL Connector configuration][mysqlconnectorconf]

## Modeling

### Data extraction from 2 different sources

1. CSV file:
    1. Add **CSV reader**;

        ![CSV Reader node][csvreader]
    1. Set the input location to your CSV file location;
    1. Check support short lines and has column header;
    1. Uncheck has row header due duplicate row IDs;
    1. Execute.

        - CSV Reader configuration:

        ![CSV Reader configuration][csvreaderconf]
2. DB Reader:
    1. Add **DB Reader**;

        ![DB Reader node][DBReader]
    1. Set the input to **DB Writer**'s connection output;
    1. Execute.

## Evaluation

### Concatenate

1. Add **Concatenate**;

    ![Concatenate node][concatenate]
1. Set **CSV Reader**'s output and the last **Json to Table**'s output as the input of **Concatenate**;
1. Execute.

The number of rows generated by the ETL equals the number of rows in the original data which means the ETL process was successful.

## Deployment

### Excel Writer

1. Add **Excel writer** with **Concatenate**'s output as input;

    ![Excel Writer node][excelwriter]
1. Set output file location;
1. Select the desired column to be included;
1. Execute.

### DB Writer

1. Add **DB writer** with **Concatenate**'s output and **Mysql Connector**'s output as input;

    ![DB Writer node][dbwriter]
1. Set output scheme and table;
1. Select the desired column to be included;
1. Execute.

## Final Workflow

![Final workflow][finalworkflow]

[csvreader]: ./images/csvreader.png "CSV Reader node"
[csvreaderconf]: ./images/csvreaderconf.png "CSV Reader configuration"
[DBReader]: ./images/DBReader.png "DB Reader node"
[mysqlconnector]: ./images/mysqlconnector.png "MySQL Connector node"
[mysqlconnectorconf]: ./images/mysqlconnectorconf.png
[dbwriter]: ./images/dbwriter.png "DB Writer node"
[concatenate]: ./images/concatenate.png "Concatenate node"
[excelwriter]: ./images/excelwriter.png "Excel Writer node"
[finalworkflow]: ./images/final.png "Final workflow"
